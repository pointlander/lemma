# Self attention based computation of principal eigenvector under certian conditions
This code generates the principal eigenvector using self attention under the conditions:
- The input vectors are positive.
- No input transforms are used.

The self attention based eigenvectors do not have the same magnitude as the principal eigenvector, only the same angle.
The angle between the eigenvectors is smaller when softmax isn't used.
See code for details.
